header:
  title: ggml
  titlePrefix: 
  permalink: 


sections:


  - sectionTag: 
    header: 
      title: Intro to ggml
      titlePrefix: 
      permalink: https://huggingface.co/blog/introduction-to-ggml#introduction-to-ggml
    content:
      syntaxHighlight: markdown
      text: |
        These notes are based on [Introduction to ggml](https://huggingface.co/blog/introduction-to-ggml).

        Main features of ```ggml``:
        - **minimalism** - The core library is self-contained in less than 5 files. While you may want to include additional files for GPU support, it's optional.
        - **easy compilation** - You don't need fancy build tools. Without GPU support, you only need GCC or Clang!
        - **lightweight** - The compiled binary size is less than 1MB, which is tiny compared to PyTorch (which usually takes hundreds of MB).
        - **good compatibility** - It supports many types of hardware, including x86_64, ARM, Apple Silicon, CUDA, etc.
        - **support for quantized tensors** - Tensors can be quantized to save memory (similar to JPEG compression) and in certain cases to improve performance.
        - **extremely memory efficient** - Overhead for storing tensors and performing computations is minimal.

        We'll explore the core concepts and basic usage of ggml to provide a solid foundation for further learning and development.

        
  - sectionTag: Terminology and concepts
    header: 
      title: Terminology and concepts
      titlePrefix: 
      permalink: https://huggingface.co/blog/introduction-to-ggml#terminology-and-concepts
    content:
      syntaxHighlight: markdown
      text: |
        ```ggml``` is *low-level* library. Let's explore basic terminology and concepts:

        - **ggml_context** - A "container" that holds objects such as tensors, graphs, and optionally data
        - **ggml_cgraph** - Represents a computational graph. Think of it as the "order of computation" that will be transferred to the backend.
        - **ggml_backend** - Represents an interface for executing computation graphs. There are many types of backends: CPU (default), CUDA, Metal (Apple Silicon), Vulkan, RPC, etc.
        - **ggml_backend_buffer_type** - Represents a buffer type. Think of it as a "memory allocator" connected to each ggml_backend. For example, if you want to perform calculations on a GPU, you need to allocate memory on the GPU via buffer_type (usually abbreviated as buft).
        - **ggml_backend_buffer** - Represents a buffer allocated by buffer_type. Remember: a buffer can hold the data of multiple tensors.
        - **ggml_gallocr** - Represents a graph memory allocator, used to allocate efficiently the tensors used in a computation graph.
        - **ggml_backend_sched** - A scheduler that enables concurrent use of multiple backends. It can distribute computations across different hardware (e.g., GPU and CPU) when dealing with large models or multiple GPUs. The scheduler can also automatically assign GPU-unsupported operations to the CPU, ensuring optimal resource utilization and compatibility.


  - sectionTag: 
    header: 
      title: Simple example in Python
      titlePrefix:
      permalink: https://huggingface.co/blog/introduction-to-ggml#simple-example
    links:
    content:
      syntaxHighlight: python
      text: |
        import torch

        # Create two matrices
        matrix1 = torch.tensor([
          [2, 8],
          [5, 1],
          [4, 2],
          [8, 6],
        ])
        matrix2 = torch.tensor([
          [10, 5],
          [9, 9],
          [5, 4],
        ])

        # Perform matrix multiplication
        result = torch.matmul(matrix1, matrix2.T)
        print(result.T)


  - sectionTag: 
    header: 
      title: Simple example in ggml
      titlePrefix:
      permalink: https://huggingface.co/blog/introduction-to-ggml#simple-example
    content:
      syntaxHighlight: markdown
      text: |
        ```Simple example in Python``` translates into low-level ggml implementation as follows:


  - sectionTag: 
    links:
      - matchString: ggml_tensor
        linkFile: ggml_tensor.yaml
        matchIndex: []
    content:
      syntaxHighlight: cpp
      text: |
        #include "ggml.h"
        #include <string.h>
        #include <stdio.h>

        int main(void) {
            // initialize data of matrices to perform matrix multiplication
            const int rows_A = 4, cols_A = 2;
            float matrix_A[rows_A * cols_A] = {
                2, 8,
                5, 1,
                4, 2,
                8, 6
            };
            const int rows_B = 3, cols_B = 2;
            float matrix_B[rows_B * cols_B] = {
                10, 5,
                9, 9,
                5, 4
            };

            // 1. Allocate `ggml_context` to store tensor data
            // Calculate the size needed to allocate
            size_t ctx_size = 0;
            ctx_size += rows_A * cols_A * ggml_type_size(GGML_TYPE_F32); // tensor a
            ctx_size += rows_B * cols_B * ggml_type_size(GGML_TYPE_F32); // tensor b
            ctx_size += rows_A * rows_B * ggml_type_size(GGML_TYPE_F32); // result
            ctx_size += 3 * ggml_tensor_overhead(); // metadata for 3 tensors
            ctx_size += ggml_graph_overhead(); // compute graph
            ctx_size += 1024; // some overhead (exact calculation omitted for simplicity)

            // Allocate `ggml_context` to store tensor data
            struct ggml_init_params params = {
                /*.mem_size   =*/ ctx_size,
                /*.mem_buffer =*/ NULL,
                /*.no_alloc   =*/ false,
            };
            struct ggml_context * ctx = ggml_init(params);

            // 2. Create tensors and set data
            struct ggml_tensor * tensor_a = ggml_new_tensor_2d(ctx, GGML_TYPE_F32, cols_A, rows_A);
            struct ggml_tensor * tensor_b = ggml_new_tensor_2d(ctx, GGML_TYPE_F32, cols_B, rows_B);
            memcpy(tensor_a->data, matrix_A, ggml_nbytes(tensor_a));
            memcpy(tensor_b->data, matrix_B, ggml_nbytes(tensor_b));


            // 3. Create a `ggml_cgraph` for mul_mat operation # veeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeery long extension of this liiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiine
            struct ggml_cgraph * gf = ggml_new_graph(ctx);

            // result = a*b^T
            // Pay attention: ggml_mul_mat(A, B) ==> B will be transposed internally
            // the result is transposed
            struct ggml_tensor * result = ggml_mul_mat(ctx, tensor_a, tensor_b);

            // Mark the "result" tensor to be computed
            ggml_build_forward_expand(gf, result);

            // 4. Run the computation
            int n_threads = 1; // Optional: number of threads to perform some operations with multi-threading
            ggml_graph_compute_with_ctx(ctx, gf, n_threads);

            // 5. Retrieve results (output tensors)
            float * result_data = (float *) result->data;
            printf("mul mat (%d x %d) (transposed result):\n[", (int) result->ne[0], (int) result->ne[1]);
            for (int j = 0; j < result->ne[1] /* rows */; j++) {
                if (j > 0) {
                    printf("\n");
                }

                for (int i = 0; i < result->ne[0] /* cols */; i++) {
                    printf(" %.2f", result_data[j * result->ne[0] + i]);
                }
            }
            printf(" ]\n");

            // 6. Free memory and exit
            ggml_free(ctx);
            return 0;
        }
