<div class="page">
<script type="application/json" id="page-data">
{"pageTitle": "common.h"}
</script>
<div class="page-header"><span class="header-title">common.h</span></div><div class="page-content">
<div class="section">
<div class="section-header">
<span class="header-title-prefix">/llama.cpp/common/</span><span class="header-title">common.h</span><a href="https://github.com/ggerganov/llama.cpp/blob/acb2c32c336ce60d765bb189563cc216e57e9fc2/common/common.h#L144" target="_blank" class="header-permalink">link</a>
</div>
<div class="section-content">
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span></pre></div></td><td class="code"><div><pre><span></span><span class="k">struct</span><span class="w"> </span><span class="nc">gpt_params</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">n_predict</span><span class="w">             </span><span class="o">=</span><span class="w">    </span><span class="mi">-1</span><span class="p">;</span><span class="w"> </span><span class="c1">// new tokens to predict</span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">n_ctx</span><span class="w">                 </span><span class="o">=</span><span class="w">     </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="c1">// context size</span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">n_batch</span><span class="w">               </span><span class="o">=</span><span class="w">  </span><span class="mi">2048</span><span class="p">;</span><span class="w"> </span><span class="c1">// logical batch size for prompt processing (must be &gt;=32 to use BLAS)</span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">n_ubatch</span><span class="w">              </span><span class="o">=</span><span class="w">   </span><span class="mi">512</span><span class="p">;</span><span class="w"> </span><span class="c1">// physical batch size for prompt processing (must be &gt;=32 to use BLAS)</span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">n_keep</span><span class="w">                </span><span class="o">=</span><span class="w">     </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="c1">// number of tokens to keep from initial prompt</span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">n_draft</span><span class="w">               </span><span class="o">=</span><span class="w">     </span><span class="mi">5</span><span class="p">;</span><span class="w"> </span><span class="c1">// number of tokens to draft during speculative decoding</span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">n_chunks</span><span class="w">              </span><span class="o">=</span><span class="w">    </span><span class="mi">-1</span><span class="p">;</span><span class="w"> </span><span class="c1">// max number of chunks to process (-1 = unlimited)</span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">n_parallel</span><span class="w">            </span><span class="o">=</span><span class="w">     </span><span class="mi">1</span><span class="p">;</span><span class="w"> </span><span class="c1">// number of parallel sequences to decode</span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">n_sequences</span><span class="w">           </span><span class="o">=</span><span class="w">     </span><span class="mi">1</span><span class="p">;</span><span class="w"> </span><span class="c1">// number of sequences to decode</span>
<span class="w">    </span><span class="kt">float</span><span class="w">   </span><span class="n">p_split</span><span class="w">               </span><span class="o">=</span><span class="w">  </span><span class="mf">0.1f</span><span class="p">;</span><span class="w"> </span><span class="c1">// speculative decoding split probability</span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">n_gpu_layers</span><span class="w">          </span><span class="o">=</span><span class="w">    </span><span class="mi">-1</span><span class="p">;</span><span class="w"> </span><span class="c1">// number of layers to store in VRAM (-1 - use default)</span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">n_gpu_layers_draft</span><span class="w">    </span><span class="o">=</span><span class="w">    </span><span class="mi">-1</span><span class="p">;</span><span class="w"> </span><span class="c1">// number of layers to store in VRAM for the draft model (-1 - use default)</span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">main_gpu</span><span class="w">              </span><span class="o">=</span><span class="w">     </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="c1">// the GPU that is used for scratch and small tensors</span>
<span class="w">    </span><span class="kt">float</span><span class="w">   </span><span class="n">tensor_split</span><span class="p">[</span><span class="mi">128</span><span class="p">]</span><span class="w">     </span><span class="o">=</span><span class="w">   </span><span class="p">{</span><span class="mi">0</span><span class="p">};</span><span class="w"> </span><span class="c1">// how split tensors should be distributed across GPUs</span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">grp_attn_n</span><span class="w">            </span><span class="o">=</span><span class="w">     </span><span class="mi">1</span><span class="p">;</span><span class="w"> </span><span class="c1">// group-attention factor</span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">grp_attn_w</span><span class="w">            </span><span class="o">=</span><span class="w">   </span><span class="mi">512</span><span class="p">;</span><span class="w"> </span><span class="c1">// group-attention width</span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">n_print</span><span class="w">               </span><span class="o">=</span><span class="w">    </span><span class="mi">-1</span><span class="p">;</span><span class="w"> </span><span class="c1">// print token count every n tokens (-1 = disabled)</span>
<span class="w">    </span><span class="kt">float</span><span class="w">   </span><span class="n">rope_freq_base</span><span class="w">        </span><span class="o">=</span><span class="w">  </span><span class="mf">0.0f</span><span class="p">;</span><span class="w"> </span><span class="c1">// RoPE base frequency</span>
<span class="w">    </span><span class="kt">float</span><span class="w">   </span><span class="n">rope_freq_scale</span><span class="w">       </span><span class="o">=</span><span class="w">  </span><span class="mf">0.0f</span><span class="p">;</span><span class="w"> </span><span class="c1">// RoPE frequency scaling factor</span>
<span class="w">    </span><span class="kt">float</span><span class="w">   </span><span class="n">yarn_ext_factor</span><span class="w">       </span><span class="o">=</span><span class="w"> </span><span class="mf">-1.0f</span><span class="p">;</span><span class="w"> </span><span class="c1">// YaRN extrapolation mix factor</span>
<span class="w">    </span><span class="kt">float</span><span class="w">   </span><span class="n">yarn_attn_factor</span><span class="w">      </span><span class="o">=</span><span class="w">  </span><span class="mf">1.0f</span><span class="p">;</span><span class="w"> </span><span class="c1">// YaRN magnitude scaling factor</span>
<span class="w">    </span><span class="kt">float</span><span class="w">   </span><span class="n">yarn_beta_fast</span><span class="w">        </span><span class="o">=</span><span class="w"> </span><span class="mf">32.0f</span><span class="p">;</span><span class="w"> </span><span class="c1">// YaRN low correction dim</span>
<span class="w">    </span><span class="kt">float</span><span class="w">   </span><span class="n">yarn_beta_slow</span><span class="w">        </span><span class="o">=</span><span class="w">  </span><span class="mf">1.0f</span><span class="p">;</span><span class="w"> </span><span class="c1">// YaRN high correction dim</span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">yarn_orig_ctx</span><span class="w">         </span><span class="o">=</span><span class="w">     </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="c1">// YaRN original context length</span>
<span class="w">    </span><span class="kt">float</span><span class="w">   </span><span class="n">defrag_thold</span><span class="w">          </span><span class="o">=</span><span class="w"> </span><span class="mf">-1.0f</span><span class="p">;</span><span class="w"> </span><span class="c1">// KV cache defragmentation threshold</span>

<span class="w">    </span><span class="k">struct</span><span class="w"> </span><span class="nc">cpu_params</span><span class="w"> </span><span class="n">cpuparams</span><span class="p">;</span>
<span class="w">    </span><span class="k">struct</span><span class="w"> </span><span class="nc">cpu_params</span><span class="w"> </span><span class="n">cpuparams_batch</span><span class="p">;</span>
<span class="w">    </span><span class="k">struct</span><span class="w"> </span><span class="nc">cpu_params</span><span class="w"> </span><span class="n">draft_cpuparams</span><span class="p">;</span>
<span class="w">    </span><span class="k">struct</span><span class="w"> </span><span class="nc">cpu_params</span><span class="w"> </span><span class="n">draft_cpuparams_batch</span><span class="p">;</span>

<span class="w">    </span><span class="n">ggml_backend_sched_eval_callback</span><span class="w"> </span><span class="n">cb_eval</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="w">    </span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">cb_eval_user_data</span><span class="w">                 </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>

<span class="w">    </span><span class="n">ggml_numa_strategy</span><span class="w"> </span><span class="n">numa</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">GGML_NUMA_STRATEGY_DISABLED</span><span class="p">;</span>

<span class="w">    </span><span class="k">enum</span><span class="w"> </span><span class="nc">llama_split_mode</span><span class="w">        </span><span class="n">split_mode</span><span class="w">        </span><span class="o">=</span><span class="w"> </span><span class="n">LLAMA_SPLIT_MODE_LAYER</span><span class="p">;</span><span class="w"> </span><span class="c1">// how to split the model across GPUs</span>
<span class="w">    </span><span class="k">enum</span><span class="w"> </span><span class="nc">llama_rope_scaling_type</span><span class="w"> </span><span class="n">rope_scaling_type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">LLAMA_ROPE_SCALING_TYPE_UNSPECIFIED</span><span class="p">;</span>
<span class="w">    </span><span class="k">enum</span><span class="w"> </span><span class="nc">llama_pooling_type</span><span class="w">      </span><span class="n">pooling_type</span><span class="w">      </span><span class="o">=</span><span class="w"> </span><span class="n">LLAMA_POOLING_TYPE_UNSPECIFIED</span><span class="p">;</span><span class="w"> </span><span class="c1">// pooling type for embeddings</span>
<span class="w">    </span><span class="k">enum</span><span class="w"> </span><span class="nc">llama_attention_type</span><span class="w">    </span><span class="n">attention_type</span><span class="w">    </span><span class="o">=</span><span class="w"> </span><span class="n">LLAMA_ATTENTION_TYPE_UNSPECIFIED</span><span class="p">;</span><span class="w"> </span><span class="c1">// attention type for embeddings</span>

<span class="w">    </span><span class="k">struct</span><span class="w"> </span><span class="nc">gpt_sampler_params</span><span class="w"> </span><span class="n">sparams</span><span class="p">;</span>

<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">model</span><span class="w">                </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">;</span><span class="w"> </span><span class="c1">// model path                                                    // NOLINT</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">model_draft</span><span class="w">          </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">;</span><span class="w"> </span><span class="c1">// draft model for speculative decoding                          // NOLINT</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">model_alias</span><span class="w">          </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;unknown&quot;</span><span class="p">;</span><span class="w"> </span><span class="c1">// model alias                                            // NOLINT</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">model_url</span><span class="w">            </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">;</span><span class="w"> </span><span class="c1">// model url to download                                         // NOLINT</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">hf_token</span><span class="w">             </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">;</span><span class="w"> </span><span class="c1">// HF token                                                      // NOLINT</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">hf_repo</span><span class="w">              </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">;</span><span class="w"> </span><span class="c1">// HF repo                                                       // NOLINT</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">hf_file</span><span class="w">              </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">;</span><span class="w"> </span><span class="c1">// HF file                                                       // NOLINT</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">prompt</span><span class="w">               </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">;</span><span class="w">                                                                  </span><span class="c1">// NOLINT</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">prompt_file</span><span class="w">          </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">;</span><span class="w"> </span><span class="c1">// store the external prompt file name                           // NOLINT</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">path_prompt_cache</span><span class="w">    </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">;</span><span class="w"> </span><span class="c1">// path to file for saving/loading prompt eval state             // NOLINT</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">input_prefix</span><span class="w">         </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">;</span><span class="w"> </span><span class="c1">// string to prefix user inputs with                             // NOLINT</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">input_suffix</span><span class="w">         </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">;</span><span class="w"> </span><span class="c1">// string to suffix user inputs with                             // NOLINT</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">logdir</span><span class="w">               </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">;</span><span class="w"> </span><span class="c1">// directory in which to save YAML log files                     // NOLINT</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">lookup_cache_static</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">;</span><span class="w"> </span><span class="c1">// path of static ngram cache file for lookup decoding           // NOLINT</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">lookup_cache_dynamic</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">;</span><span class="w"> </span><span class="c1">// path of dynamic ngram cache file for lookup decoding          // NOLINT</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">logits_file</span><span class="w">          </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">;</span><span class="w"> </span><span class="c1">// file for saving *all* logits                                  // NOLINT</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">rpc_servers</span><span class="w">          </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">;</span><span class="w"> </span><span class="c1">// comma separated list of RPC servers                           // NOLINT</span>

<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span><span class="w"> </span><span class="n">in_files</span><span class="p">;</span><span class="w">   </span><span class="c1">// all input files</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span><span class="w"> </span><span class="n">antiprompt</span><span class="p">;</span><span class="w"> </span><span class="c1">// strings upon which more user input is prompted (a.k.a. reverse prompts)</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">llama_model_kv_override</span><span class="o">&gt;</span><span class="w"> </span><span class="n">kv_overrides</span><span class="p">;</span>

<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">lora_init_without_apply</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// only load lora to memory, but do not apply it to ctx (user can manually apply lora later using llama_lora_adapter_apply)</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">llama_lora_adapter_info</span><span class="o">&gt;</span><span class="w"> </span><span class="n">lora_adapters</span><span class="p">;</span><span class="w"> </span><span class="c1">// lora adapter path with user defined scale</span>

<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">llama_control_vector_load_info</span><span class="o">&gt;</span><span class="w"> </span><span class="n">control_vectors</span><span class="p">;</span><span class="w"> </span><span class="c1">// control vector with user defined scale</span>

<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">verbosity</span><span class="w">                  </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">control_vector_layer_start</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span><span class="w"> </span><span class="c1">// layer range for control vector</span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">control_vector_layer_end</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span><span class="w"> </span><span class="c1">// layer range for control vector</span>

<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">ppl_stride</span><span class="w">      </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w">     </span><span class="c1">// stride for perplexity calculations. If left at 0, the pre-existing approach will be used.</span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">ppl_output_type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w">     </span><span class="c1">// = 0 -&gt; ppl output is as usual, = 1 -&gt; ppl output is num_tokens, ppl, one per line</span>
<span class="w">                                    </span><span class="c1">//                                       (which is more convenient to use for plotting)</span>
<span class="w">                                    </span><span class="c1">//</span>
<span class="w">    </span><span class="kt">bool</span><span class="w">   </span><span class="n">hellaswag</span><span class="w">        </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// compute HellaSwag score over random tasks from datafile supplied in prompt</span>
<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">hellaswag_tasks</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="mi">400</span><span class="p">;</span><span class="w">   </span><span class="c1">// number of tasks to use when computing the HellaSwag score</span>

<span class="w">    </span><span class="kt">bool</span><span class="w">   </span><span class="n">winogrande</span><span class="w">       </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// compute Winogrande score over random tasks from datafile supplied in prompt</span>
<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">winogrande_tasks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w">     </span><span class="c1">// number of tasks to use when computing the Winogrande score. If 0, all tasks will be computed</span>

<span class="w">    </span><span class="kt">bool</span><span class="w">   </span><span class="n">multiple_choice</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w">  </span><span class="c1">// compute TruthfulQA score over random tasks from datafile supplied in prompt</span>
<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">multiple_choice_tasks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="c1">// number of tasks to use when computing the TruthfulQA score. If 0, all tasks will be computed</span>

<span class="w">    </span><span class="kt">bool</span><span class="w">   </span><span class="n">kl_divergence</span><span class="w">    </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// compute KL divergence</span>

<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">usage</span><span class="w">             </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// print usage</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">use_color</span><span class="w">         </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// use color to distinguish generations and inputs</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">special</span><span class="w">           </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// enable special token output</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">interactive</span><span class="w">       </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// interactive mode</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">interactive_first</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// wait for user input immediately</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">conversation</span><span class="w">      </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// conversation mode (does not print special tokens and suffix/prefix)</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">prompt_cache_all</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// save user input and generations to prompt cache</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">prompt_cache_ro</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// open the prompt cache read-only and do not update it</span>

<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">escape</span><span class="w">            </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span><span class="w">  </span><span class="c1">// escape &quot;\n&quot;, &quot;\r&quot;, &quot;\t&quot;, &quot;\&#39;&quot;, &quot;\&quot;&quot;, and &quot;\\&quot;</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">multiline_input</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// reverse the usage of `\`</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">simple_io</span><span class="w">         </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// improves compatibility with subprocesses and limited consoles</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">cont_batching</span><span class="w">     </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span><span class="w">  </span><span class="c1">// insert new sequences for decoding on-the-fly</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">flash_attn</span><span class="w">        </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// flash attention</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">no_perf</span><span class="w">           </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// disable performance metrics</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">ctx_shift</span><span class="w">         </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span><span class="w">  </span><span class="c1">// context shift on inifinite text generation</span>

<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">input_prefix_bos</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// prefix BOS to user inputs, preceding input_prefix</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">logits_all</span><span class="w">        </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// return logits for all tokens in the batch</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">use_mmap</span><span class="w">          </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span><span class="w">  </span><span class="c1">// use mmap for faster loads</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">use_mlock</span><span class="w">         </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// use mlock to keep model in memory</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">verbose_prompt</span><span class="w">    </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// print prompt tokens before generation</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">display_prompt</span><span class="w">    </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span><span class="w">  </span><span class="c1">// print prompt before generation</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">dump_kv_cache</span><span class="w">     </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// dump the KV cache contents for debugging purposes</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">no_kv_offload</span><span class="w">     </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// disable KV offloading</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">warmup</span><span class="w">            </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span><span class="w">  </span><span class="c1">// warmup run</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">check_tensors</span><span class="w">     </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// validate tensor data</span>

<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">cache_type_k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;f16&quot;</span><span class="p">;</span><span class="w"> </span><span class="c1">// KV cache data type for the K</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">cache_type_v</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;f16&quot;</span><span class="p">;</span><span class="w"> </span><span class="c1">// KV cache data type for the V</span>

<span class="w">    </span><span class="c1">// multimodal models (see examples/llava)</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">mmproj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">;</span><span class="w">        </span><span class="c1">// path to multimodal projector                                         // NOLINT</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span><span class="w"> </span><span class="n">image</span><span class="p">;</span><span class="w"> </span><span class="c1">// path to image file(s)</span>

<span class="w">    </span><span class="c1">// embedding</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">embedding</span><span class="w">         </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// get only sentence embedding</span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">embd_normalize</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span><span class="w">     </span><span class="c1">// normalisation for embendings (-1=none, 0=max absolute int16, 1=taxicab, 2=euclidean, &gt;2=p-norm)</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">embd_out</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">;</span><span class="w">    </span><span class="c1">// empty = default, &quot;array&quot; = [[],[]...], &quot;json&quot; = openai style, &quot;json+&quot; = same &quot;json&quot; + cosine similarity matrix</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">embd_sep</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span><span class="w">  </span><span class="c1">// separator of embendings</span>

<span class="w">    </span><span class="c1">// server params</span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">port</span><span class="w">           </span><span class="o">=</span><span class="w"> </span><span class="mi">8080</span><span class="p">;</span><span class="w">         </span><span class="c1">// server listens on this network port</span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">timeout_read</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="mi">600</span><span class="p">;</span><span class="w">          </span><span class="c1">// http read timeout in seconds</span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">timeout_write</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="n">timeout_read</span><span class="p">;</span><span class="w"> </span><span class="c1">// http write timeout in seconds</span>
<span class="w">    </span><span class="kt">int</span><span class="w">     </span><span class="n">n_threads_http</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span><span class="w">           </span><span class="c1">// number of threads to process HTTP requests (TODO: support threadpool)</span>

<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">hostname</span><span class="w">      </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;127.0.0.1&quot;</span><span class="p">;</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">public_path</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">;</span><span class="w">                                                                         </span><span class="c1">// NOLINT</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">chat_template</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">;</span><span class="w">                                                                         </span><span class="c1">// NOLINT</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">system_prompt</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">;</span><span class="w">                                                                         </span><span class="c1">// NOLINT</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">enable_chat_template</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>

<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span><span class="w"> </span><span class="n">api_keys</span><span class="p">;</span>

<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">ssl_file_key</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">;</span><span class="w">                                                                         </span><span class="c1">// NOLINT</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">ssl_file_cert</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">;</span><span class="w">                                                                         </span><span class="c1">// NOLINT</span>

<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">endpoint_slots</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">endpoint_metrics</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>

<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">log_json</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>

<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">slot_save_path</span><span class="p">;</span>

<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">slot_prompt_similarity</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.5f</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// batched-bench params</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">is_pp_shared</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>

<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">n_pp</span><span class="p">;</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">n_tg</span><span class="p">;</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int32_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">n_pl</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// retrieval params</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&gt;</span><span class="w"> </span><span class="n">context_files</span><span class="p">;</span><span class="w"> </span><span class="c1">// context files to embed</span>

<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">chunk_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">64</span><span class="p">;</span><span class="w"> </span><span class="c1">// chunk size for context embedding</span>

<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">chunk_separator</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">;</span><span class="w"> </span><span class="c1">// chunk separator for context embedding</span>

<span class="w">    </span><span class="c1">// passkey params</span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">n_junk</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">250</span><span class="p">;</span><span class="w"> </span><span class="c1">// number of times to repeat the junk text</span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">i_pos</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="mi">-1</span><span class="p">;</span><span class="w">  </span><span class="c1">// position of the passkey in the junk text</span>

<span class="w">    </span><span class="c1">// imatrix params</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">out_file</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;imatrix.dat&quot;</span><span class="p">;</span><span class="w"> </span><span class="c1">// save the resulting imatrix to this file</span>

<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">n_out_freq</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="mi">10</span><span class="p">;</span><span class="w"> </span><span class="c1">// output the imatrix every n_out_freq iterations</span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">n_save_freq</span><span class="w"> </span><span class="o">=</span><span class="w">  </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="c1">// save the imatrix every n_save_freq iterations</span>
<span class="w">    </span><span class="kt">int32_t</span><span class="w"> </span><span class="n">i_chunk</span><span class="w">     </span><span class="o">=</span><span class="w">  </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="c1">// start processing from this chunk</span>

<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">process_output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// collect data for the output tensor</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">compute_ppl</span><span class="w">    </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span><span class="w">  </span><span class="c1">// whether to compute perplexity</span>

<span class="w">    </span><span class="c1">// cvector-generator params</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">n_pca_batch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">100</span><span class="p">;</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">n_pca_iterations</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1000</span><span class="p">;</span>
<span class="w">    </span><span class="n">dimre_method</span><span class="w"> </span><span class="n">cvector_dimre_method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">DIMRE_METHOD_PCA</span><span class="p">;</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">cvector_outfile</span><span class="w">       </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;control_vector.gguf&quot;</span><span class="p">;</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">cvector_positive_file</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;examples/cvector-generator/positive.txt&quot;</span><span class="p">;</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">cvector_negative_file</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;examples/cvector-generator/negative.txt&quot;</span><span class="p">;</span>

<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">spm_infill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w"> </span><span class="c1">// suffix/prefix/middle pattern for infill</span>

<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">lora_outfile</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;ggml-lora-merged-f16.gguf&quot;</span><span class="p">;</span>

<span class="w">    </span><span class="c1">// batched-bench params</span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">batched_bench_output_jsonl</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span>
<span class="p">};</span>
</pre></div></td></tr></table></div>

</div>
</div>
</div>
</div>
