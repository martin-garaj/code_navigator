<div class="page">
<script type="application/json" id="page-data">
{"pageTitle": "ggml"}
</script>
<div class="page-header"><span class="header-title">ggml</span></div><div class="page-content">
<div class="section">
<div class="section-header">
<span class="header-title">Intro to ggml</span><a href="https://huggingface.co/blog/introduction-to-ggml#introduction-to-ggml" target="_blank" class="header-permalink">link</a>
</div>
<div class="section-content">
<div class="highlight"><pre><span></span>These notes are based on [<span class="nt">Introduction to ggml</span>](<span class="na">https://huggingface.co/blog/introduction-to-ggml</span>).

Main features of ```ggml``:
<span class="k">-</span><span class="w"> </span><span class="gs">**minimalism**</span> - The core library is self-contained in less than 5 files. While you may want to include additional files for GPU support, it&#39;s optional.
<span class="k">-</span><span class="w"> </span><span class="gs">**easy compilation**</span> - You don&#39;t need fancy build tools. Without GPU support, you only need GCC or Clang!
<span class="k">-</span><span class="w"> </span><span class="gs">**lightweight**</span> - The compiled binary size is less than 1MB, which is tiny compared to PyTorch (which usually takes hundreds of MB).
<span class="k">-</span><span class="w"> </span><span class="gs">**good compatibility**</span> - It supports many types of hardware, including x86_64, ARM, Apple Silicon, CUDA, etc.
<span class="k">-</span><span class="w"> </span><span class="gs">**support for quantized tensors**</span> - Tensors can be quantized to save memory (similar to JPEG compression) and in certain cases to improve performance.
<span class="k">-</span><span class="w"> </span><span class="gs">**extremely memory efficient**</span> - Overhead for storing tensors and performing computations is minimal.

We&#39;ll explore the core concepts and basic usage of ggml to provide a solid foundation for further learning and development.
</pre></div>

</div>
</div>
<div class="section">
<div class="section-header">
<span class="header-title">Terminology and concepts</span><a href="https://huggingface.co/blog/introduction-to-ggml#terminology-and-concepts" target="_blank" class="header-permalink">link</a>
</div>
<div class="section-content">
<div class="highlight"><pre><span></span>```ggml``` is <span class="ge">*low-level*</span> library. Let&#39;s explore basic terminology and concepts:

<span class="k">-</span><span class="w"> </span><span class="gs">**ggml_context**</span> - A &quot;container&quot; that holds objects such as tensors, graphs, and optionally data
<span class="k">-</span><span class="w"> </span><span class="gs">**ggml_cgraph**</span> - Represents a computational graph. Think of it as the &quot;order of computation&quot; that will be transferred to the backend.
<span class="k">-</span><span class="w"> </span><span class="gs">**ggml_backend**</span> - Represents an interface for executing computation graphs. There are many types of backends: CPU (default), CUDA, Metal (Apple Silicon), Vulkan, RPC, etc.
<span class="k">-</span><span class="w"> </span><span class="gs">**ggml_backend_buffer_type**</span> - Represents a buffer type. Think of it as a &quot;memory allocator&quot; connected to each ggml_backend. For example, if you want to perform calculations on a GPU, you need to allocate memory on the GPU via buffer_type (usually abbreviated as buft).
<span class="k">-</span><span class="w"> </span><span class="gs">**ggml_backend_buffer**</span> - Represents a buffer allocated by buffer_type. Remember: a buffer can hold the data of multiple tensors.
<span class="k">-</span><span class="w"> </span><span class="gs">**ggml_gallocr**</span> - Represents a graph memory allocator, used to allocate efficiently the tensors used in a computation graph.
<span class="k">-</span><span class="w"> </span><span class="gs">**ggml_backend_sched**</span> - A scheduler that enables concurrent use of multiple backends. It can distribute computations across different hardware (e.g., GPU and CPU) when dealing with large models or multiple GPUs. The scheduler can also automatically assign GPU-unsupported operations to the CPU, ensuring optimal resource utilization and compatibility.
</pre></div>

</div>
</div>
<div class="section">
<div class="section-header">
<span class="header-title">Simple example in Python</span><a href="https://huggingface.co/blog/introduction-to-ggml#simple-example" target="_blank" class="header-permalink">link</a>
</div>
<div class="section-content">
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span></pre></div></td><td class="code"><div><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># Create two matrices</span>
<span class="n">matrix1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
  <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
  <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
  <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
  <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
<span class="p">])</span>
<span class="n">matrix2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span>
  <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
  <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span>
  <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
<span class="p">])</span>

<span class="c1"># Perform matrix multiplication</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">matrix1</span><span class="p">,</span> <span class="n">matrix2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div></td></tr></table></div>

</div>
</div>
<div class="section">
<div class="section-header">
<span class="header-title">Simple example in ggml</span><a href="https://huggingface.co/blog/introduction-to-ggml#simple-example" target="_blank" class="header-permalink">link</a>
</div>
<div class="section-content">
<div class="highlight"><pre><span></span>```Simple example in Python``` translates into low-level ggml implementation as follows:
</pre></div>

</div>
</div>
<div class="section">
<div class="section-header">
</div>
<div class="section-content">
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span></pre></div></td><td class="code"><div><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;ggml.h&quot;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;string.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// initialize data of matrices to perform matrix multiplication</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">rows_A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="n">cols_A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">matrix_A</span><span class="p">[</span><span class="n">rows_A</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">cols_A</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span>
<span class="w">        </span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<span class="w">        </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span>
<span class="w">        </span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">6</span>
<span class="w">    </span><span class="p">};</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">rows_B</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="n">cols_B</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="n">matrix_B</span><span class="p">[</span><span class="n">rows_B</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">cols_B</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span>
<span class="w">        </span><span class="mi">9</span><span class="p">,</span><span class="w"> </span><span class="mi">9</span><span class="p">,</span>
<span class="w">        </span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span>
<span class="w">    </span><span class="p">};</span>

<span class="w">    </span><span class="c1">// 1. Allocate `ggml_context` to store tensor data</span>
<span class="w">    </span><span class="c1">// Calculate the size needed to allocate</span>
<span class="w">    </span><span class="kt">size_t</span><span class="w"> </span><span class="n">ctx_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">    </span><span class="n">ctx_size</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">rows_A</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">cols_A</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">ggml_type_size</span><span class="p">(</span><span class="n">GGML_TYPE_F32</span><span class="p">);</span><span class="w"> </span><span class="c1">// tensor a</span>
<span class="w">    </span><span class="n">ctx_size</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">rows_B</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">cols_B</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">ggml_type_size</span><span class="p">(</span><span class="n">GGML_TYPE_F32</span><span class="p">);</span><span class="w"> </span><span class="c1">// tensor b</span>
<span class="w">    </span><span class="n">ctx_size</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">rows_A</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">rows_B</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">ggml_type_size</span><span class="p">(</span><span class="n">GGML_TYPE_F32</span><span class="p">);</span><span class="w"> </span><span class="c1">// result</span>
<span class="w">    </span><span class="n">ctx_size</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">ggml_tensor_overhead</span><span class="p">();</span><span class="w"> </span><span class="c1">// metadata for 3 tensors</span>
<span class="w">    </span><span class="n">ctx_size</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">ggml_graph_overhead</span><span class="p">();</span><span class="w"> </span><span class="c1">// compute graph</span>
<span class="w">    </span><span class="n">ctx_size</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1024</span><span class="p">;</span><span class="w"> </span><span class="c1">// some overhead (exact calculation omitted for simplicity)</span>

<span class="w">    </span><span class="c1">// Allocate `ggml_context` to store tensor data</span>
<span class="w">    </span><span class="k">struct</span><span class="w"> </span><span class="nc">ggml_init_params</span><span class="w"> </span><span class="n">params</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="cm">/*.mem_size   =*/</span><span class="w"> </span><span class="n">ctx_size</span><span class="p">,</span>
<span class="w">        </span><span class="cm">/*.mem_buffer =*/</span><span class="w"> </span><span class="nb">NULL</span><span class="p">,</span>
<span class="w">        </span><span class="cm">/*.no_alloc   =*/</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span>
<span class="w">    </span><span class="p">};</span>
<span class="w">    </span><span class="k">struct</span><span class="w"> </span><span class="nc">ggml_context</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">ctx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ggml_init</span><span class="p">(</span><span class="n">params</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// 2. Create tensors and set data</span>
<span class="w">    </span><span class="k">struct</span><span class="w"> </span><span class="nc"><a onclick="handleClick('ggml_tensor', 'ggml_tensor.html')" onmouseover="debouncedHandleHover('ggml_tensor.html')" class="navigator-link">ggml_tensor</a></span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">tensor_a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ggml_new_tensor_2d</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span><span class="w"> </span><span class="n">GGML_TYPE_F32</span><span class="p">,</span><span class="w"> </span><span class="n">cols_A</span><span class="p">,</span><span class="w"> </span><span class="n">rows_A</span><span class="p">);</span>
<span class="w">    </span><span class="k">struct</span><span class="w"> </span><span class="nc"><a onclick="handleClick('ggml_tensor', 'ggml_tensor.html')" onmouseover="debouncedHandleHover('ggml_tensor.html')" class="navigator-link">ggml_tensor</a></span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">tensor_b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ggml_new_tensor_2d</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span><span class="w"> </span><span class="n">GGML_TYPE_F32</span><span class="p">,</span><span class="w"> </span><span class="n">cols_B</span><span class="p">,</span><span class="w"> </span><span class="n">rows_B</span><span class="p">);</span>
<span class="w">    </span><span class="n">memcpy</span><span class="p">(</span><span class="n">tensor_a</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">matrix_A</span><span class="p">,</span><span class="w"> </span><span class="n">ggml_nbytes</span><span class="p">(</span><span class="n">tensor_a</span><span class="p">));</span>
<span class="w">    </span><span class="n">memcpy</span><span class="p">(</span><span class="n">tensor_b</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">matrix_B</span><span class="p">,</span><span class="w"> </span><span class="n">ggml_nbytes</span><span class="p">(</span><span class="n">tensor_b</span><span class="p">));</span>


<span class="w">    </span><span class="c1">// 3. Create a `ggml_cgraph` for mul_mat operation # veeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeery long extension of this liiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiine</span>
<span class="w">    </span><span class="k">struct</span><span class="w"> </span><span class="nc">ggml_cgraph</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">gf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ggml_new_graph</span><span class="p">(</span><span class="n">ctx</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// result = a*b^T</span>
<span class="w">    </span><span class="c1">// Pay attention: ggml_mul_mat(A, B) ==&gt; B will be transposed internally</span>
<span class="w">    </span><span class="c1">// the result is transposed</span>
<span class="w">    </span><span class="k">struct</span><span class="w"> </span><span class="nc"><a onclick="handleClick('ggml_tensor', 'ggml_tensor.html')" onmouseover="debouncedHandleHover('ggml_tensor.html')" class="navigator-link">ggml_tensor</a></span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ggml_mul_mat</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span><span class="w"> </span><span class="n">tensor_a</span><span class="p">,</span><span class="w"> </span><span class="n">tensor_b</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// Mark the &quot;result&quot; tensor to be computed</span>
<span class="w">    </span><span class="n">ggml_build_forward_expand</span><span class="p">(</span><span class="n">gf</span><span class="p">,</span><span class="w"> </span><span class="n">result</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// 4. Run the computation</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">n_threads</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span><span class="w"> </span><span class="c1">// Optional: number of threads to perform some operations with multi-threading</span>
<span class="w">    </span><span class="n">ggml_graph_compute_with_ctx</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span><span class="w"> </span><span class="n">gf</span><span class="p">,</span><span class="w"> </span><span class="n">n_threads</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// 5. Retrieve results (output tensors)</span>
<span class="w">    </span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">result_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="kt">float</span><span class="w"> </span><span class="o">*</span><span class="p">)</span><span class="w"> </span><span class="n">result</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">;</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;mul mat (%d x %d) (transposed result):</span><span class="se">\n</span><span class="s">[&quot;</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="w"> </span><span class="n">result</span><span class="o">-&gt;</span><span class="n">ne</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="p">)</span><span class="w"> </span><span class="n">result</span><span class="o">-&gt;</span><span class="n">ne</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">result</span><span class="o">-&gt;</span><span class="n">ne</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="w"> </span><span class="cm">/* rows */</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">j</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>

<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">result</span><span class="o">-&gt;</span><span class="n">ne</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="cm">/* cols */</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot; %.2f&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">result_data</span><span class="p">[</span><span class="n">j</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">result</span><span class="o">-&gt;</span><span class="n">ne</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="p">]);</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot; ]</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>

<span class="w">    </span><span class="c1">// 6. Free memory and exit</span>
<span class="w">    </span><span class="n">ggml_free</span><span class="p">(</span><span class="n">ctx</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></div></td></tr></table></div>

</div>
</div>
</div>
</div>
